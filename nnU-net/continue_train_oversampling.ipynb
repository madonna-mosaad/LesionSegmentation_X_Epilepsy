{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Environment Setup\n",
    "Installing dependencies and importing required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install nnunetv2\n",
    "\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "print(\"\u2713 Dependencies installed and imported\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Experiment Configuration\n",
    "Setting up paths, environment variables, and defining rare subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training configuration - Fold Selection\n",
    "FOLD = 0  # Select fold (0, 1, 2, 3, 4)\n",
    "CHECKPOINT_PATH = f\"Enter your checkpoint path here\" # Define your Checkpoint paths\n\n",
    "# ------------------------------------------\n",
    "# User Programmable Parameters\n",
    "# ------------------------------------------\n",
    "TRAINING_TIME_MINUTES = (11 * 60) + 45  # 11 hours 45 minutes\n",
    "OVERSAMPLE_FACTOR = 3.0\n",
    "\n",
    "# Define your dataset paths - MODIFY THESE TO MATCH YOUR KAGGLE PATHS\n",
    "PREPROCESSED_PATH = \"/kaggle/input/preprocessed-bonnfcd-flair/nnUNet_preprocessed/Dataset002_BonnFCD_FLAIR\"\n",
    "RAW_PATH = \"/kaggle/input/preprocessed-bonnfcd-flair/nnUNet_raw_data_base/nnUNet_raw/Dataset002_BonnFCD_FLAIR\"\n",
    "\n",
    "# nnUNet environment variables\n",
    "os.environ['nnUNet_raw'] = \"/kaggle/input/preprocessed-bonnfcd-flair/nnUNet_raw_data_base/nnUNet_raw\"\n",
    "os.environ['nnUNet_preprocessed'] = \"/kaggle/input/preprocessed-bonnfcd-flair/nnUNet_preprocessed\"\n",
    "os.environ['nnUNet_results'] = \"/kaggle/working/nnUNet_results\"\n",
    "if 'nnUNet_compile' not in os.environ:\n",
    "    os.environ['nnUNet_compile'] = 'false' # Set default\n",
    "\n",
    "# ------------------------------------------\n",
    "# Configuration for Custom Modules\n",
    "# ------------------------------------------\n",
    "# Define the path to the custom modules directory\n",
    "CUSTOM_MODULES_PATH = \"custom_modules\"\n",
    "RARE_SUBJECTS_PATH = os.path.join(CUSTOM_MODULES_PATH, \"rare_subjects.json\")\n",
    "SPLITS_PATH = os.path.join(CUSTOM_MODULES_PATH, \"splits_final.json\")\n",
    "\n",
    "# Load rare subjects for validation checks\n",
    "with open(RARE_SUBJECTS_PATH, \"r\") as f:\n",
    "    RARE_SUBJECTS = json.load(f)\n",
    "\n",
    "# CREATE TRAINER CONFIG JSON\n",
    "trainer_config = {\n",
    "    \"oversample_factor\": OVERSAMPLE_FACTOR,\n",
    "    \"max_time_minutes\": TRAINING_TIME_MINUTES\n",
    "}\n",
    "config_path = os.path.join(CUSTOM_MODULES_PATH, \"trainer_config.json\")\n",
    "with open(config_path, \"w\") as f:\n",
    "    json.dump(trainer_config, f, indent=4)\n",
    "\n",
    "print(f\"\u2713 Configuration set\")\n",
    "print(f\"  - Custom modules path: {CUSTOM_MODULES_PATH}\")\n",
    "print(f\"  - Trainer config created at: {config_path}\")\n",
    "print(f\"  - Oversample factor: {OVERSAMPLE_FACTOR}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Custom nnU-Net Components\n",
    "Defining the custom DataLoader and Trainer classes to handle oversampling and safe training limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_custom_dataloader_file():\n",
    "    \"\"\"\n",
    "    Copy the custom DataLoader from custom_modules to the working directory\n",
    "    \"\"\"\n",
    "    source = os.path.join(CUSTOM_MODULES_PATH, \"custom_dataloader.py\")\n",
    "    target_dir = \"/kaggle/working/custom_nnunet\"\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    target = os.path.join(target_dir, \"custom_dataloader.py\")\n",
    "    \n",
    "    shutil.copy2(source, target)\n",
    "    print(f\"\u2713 Custom DataLoader copied from {source} to {target}\")\n",
    "\n",
    "create_custom_dataloader_file()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import nnunetv2\n",
    "from pathlib import Path\n",
    "\n",
    "def create_custom_trainer():\n",
    "    \"\"\"\n",
    "    Copy the custom Trainer and config from custom_modules to:\n",
    "    1. The working directory (for reference/import)\n",
    "    2. The nnU-Net package directory (so it can be used by -tr argument)\n",
    "    \"\"\"\n",
    "    target_dir = \"/kaggle/working/custom_nnunet\"\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "    # Files to copy\n",
    "    files_to_copy = [\n",
    "        \"nnUNetTrainerOversampling.py\",\n",
    "        \"rare_subjects.json\",\n",
    "        \"trainer_config.json\"\n",
    "    ]\n",
    "    \n",
    "    # 1. Copy to working dir\n",
    "    for file in files_to_copy:\n",
    "        src = os.path.join(CUSTOM_MODULES_PATH, file)\n",
    "        dst = os.path.join(target_dir, file)\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy2(src, dst)\n",
    "        else:\n",
    "            print(f\"\u26a0 Warning: {src} not found!\")\n",
    "\n",
    "    # 2. Copy to nnU-Net package location\n",
    "    nnunet_trainers_dir = Path(nnunetv2.__file__).parent / \"training\" / \"nnUNetTrainer\"\n",
    "    \n",
    "    for file in files_to_copy:\n",
    "        src = os.path.join(CUSTOM_MODULES_PATH, file)\n",
    "        dst = nnunet_trainers_dir / file\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy2(src, dst)\n",
    "            \n",
    "    print(f\"\u2713 Custom Trainer deployed to:\")\n",
    "    print(f\"  - {target_dir}\")\n",
    "    print(f\"  - {nnunet_trainers_dir}\")\n",
    "\n",
    "create_custom_trainer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Preparation\n",
    "Copying the preprocessed dataset to the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "source = \"/kaggle/input/preprocessed-bonnfcd-flair/nnUNet_preprocessed/Dataset002_BonnFCD_FLAIR\"\n",
    "dest = \"/kaggle/working/nnUNet_preprocessed/Dataset002_BonnFCD_FLAIR\"\n",
    "\n",
    "print(\"Copying preprocessed data to writable location...\")\n",
    "print(f\"From: {source}\")\n",
    "print(f\"To: {dest}\")\n",
    "\n",
    "Path(dest).parent.mkdir(parents=True, exist_ok=True)\n",
    "shutil.copytree(source, dest, dirs_exist_ok=True)\n",
    "\n",
    "print(f\"\u2713 Copied successfully\")\n",
    "os.environ['nnUNet_preprocessed'] = \"/kaggle/working/nnUNet_preprocessed\"\n",
    "print(f\"\u2713 Updated nnUNet_preprocessed path\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Advanced Customization (Augmentation & Splits)\n",
    "Applying doubled data augmentation parameters and enforcing fixed cross-validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "\n",
    "plans_path = \"/kaggle/working/nnUNet_preprocessed/Dataset002_BonnFCD_FLAIR/nnUNetPlans.json\"\n",
    "\n",
    "# Load plans\n",
    "with open(plans_path, \"r\") as f:\n",
    "    plans = json.load(f)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Ensure the augmentation keys exist\n",
    "# ----------------------------------------------------\n",
    "if \"data_augmentation\" not in plans:\n",
    "    plans[\"data_augmentation\"] = {}\n",
    "\n",
    "if \"spatial\" not in plans[\"data_augmentation\"]:\n",
    "    plans[\"data_augmentation\"][\"spatial\"] = {}\n",
    "\n",
    "if \"intensity\" not in plans[\"data_augmentation\"]:\n",
    "    plans[\"data_augmentation\"][\"intensity\"] = {}\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# \ud83d\udd25 Doubled spatial augmentations\n",
    "# ----------------------------------------------------\n",
    "# Rotation: \u00b160 degrees (Default was 30)\n",
    "plans[\"data_augmentation\"][\"spatial\"][\"rotation\"] = {\n",
    "    \"x\": 60 * (math.pi / 180),\n",
    "    \"y\": 60 * (math.pi / 180),\n",
    "    \"z\": 60 * (math.pi / 180)\n",
    "}\n",
    "\n",
    "# Scale: Range [0.70, 1.50] (Default was [0.85, 1.25])\n",
    "# Logic: Doubled the deviation from 1.0 (15%->30% down, 25%->50% up)\n",
    "plans[\"data_augmentation\"][\"spatial\"][\"scale_range\"] = [0.70, 1.50]\n",
    "\n",
    "# Elastic Deformation: Still Disabled (Doubling None is None)\n",
    "# If you want to force it on, you must add a dictionary here.\n",
    "plans[\"data_augmentation\"][\"spatial\"][\"elastic_deform\"] = None\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# \ud83d\udd25 Doubled intensity augmentations\n",
    "# ----------------------------------------------------\n",
    "# Brightness: Range [0.5, 1.5] (Default was [0.75, 1.25])\n",
    "plans[\"data_augmentation\"][\"intensity\"][\"brightness\"] = [0.5, 1.5]\n",
    "\n",
    "# Contrast: Range [0.5, 1.5] (Default was [0.75, 1.25])\n",
    "plans[\"data_augmentation\"][\"intensity\"][\"contrast\"] = [0.5, 1.5]\n",
    "\n",
    "# Gaussian Noise: Variance 0.2 (Default was 0.1)\n",
    "plans[\"data_augmentation\"][\"intensity\"][\"gaussian_noise_std\"] = 0.2\n",
    "\n",
    "# Gamma: Range [0.4, 2.0] (Default was [0.7, 1.5])\n",
    "# Logic: Doubled the deviation from 1.0\n",
    "plans[\"data_augmentation\"][\"intensity\"][\"gamma_range\"] = [0.4, 2.0]\n",
    "\n",
    "# Save back\n",
    "with open(plans_path, \"w\") as f:\n",
    "    json.dump(plans, f, indent=4)\n",
    "\n",
    "print(\"\ud83d\udd25 Augmentation parameters DOUBLED successfully!\")\n",
    "print(\"- Rotation: \u00b160\u00b0\")\n",
    "print(\"- Scale: 0.70 - 1.50\")\n",
    "print(\"- Noise/Contrast/Gamma: Deviation doubled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# \ud83e\uddea Custom 5-Fold Cross-Validation Splits\n",
    "# ----------------------------------------------------\n",
    "# Load splits from JSON\n",
    "with open(SPLITS_PATH, \"r\") as f:\n",
    "    custom_splits = json.load(f)\n",
    "\n",
    "# Define the target path where nnU-Net expects the splits file\n",
    "# It must be in the preprocessed directory of the specific task\n",
    "preprocessed_dir = \"/kaggle/working/nnUNet_preprocessed/Dataset002_BonnFCD_FLAIR\"\n",
    "splits_file = os.path.join(preprocessed_dir, \"splits_final.json\")\n",
    "\n",
    "# Write the splits to the file\n",
    "os.makedirs(preprocessed_dir, exist_ok=True)\n",
    "with open(splits_file, \"w\") as f:\n",
    "    json.dump(custom_splits, f, indent=4)\n",
    "\n",
    "print(f\"\u2705 Custom splits enforced successfully!\")\n",
    "print(f\"  - Loaded from: {SPLITS_PATH}\")\n",
    "print(f\"  - Written to: {splits_file}\")\n",
    "print(f\"  - Total folds: {len(custom_splits)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Checkpoint Restoration\n",
    "Copying the previous checkpoint logic to resume training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COPYING PREVIOUS CHECKPOINT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create results directory structure\n",
    "results_dir = Path(f\"/kaggle/working/nnUNet_results/Dataset002_BonnFCD_FLAIR/nnUNetTrainerOversampling__nnUNetPlans__3d_fullres/fold_{FOLD}\")\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define source\n",
    "checkpoint_source = Path(CHECKPOINT_PATH)\n",
    "\n",
    "if checkpoint_source.exists():\n",
    "    print(f\"\\n\u2713 Found checkpoint source: {checkpoint_source}\")\n",
    "\n",
    "    # CASE 1: user provided a specific FILE (e.g., checkpoint_final.pth)\n",
    "    if checkpoint_source.is_file():\n",
    "        # Rename it to what nnU-Net expects for resuming: 'checkpoint_latest.pth'\n",
    "        dest_name = \"checkpoint_latest.pth\"\n",
    "        dest_item = results_dir / dest_name\n",
    "        shutil.copy2(checkpoint_source, dest_item)\n",
    "        print(f\"  \u2713 Copied file: {checkpoint_source.name} -> {dest_name}\")\n",
    "\n",
    "    # CASE 2: user provided a DIRECTORY\n",
    "    elif checkpoint_source.is_dir():\n",
    "        # Copy all files from that directory\n",
    "        for item in checkpoint_source.iterdir():\n",
    "            dest_item = results_dir / item.name\n",
    "            if item.is_file():\n",
    "                shutil.copy2(item, dest_item)\n",
    "                print(f\"  \u2713 Copied: {item.name}\")\n",
    "            elif item.is_dir():\n",
    "                shutil.copytree(item, dest_item, dirs_exist_ok=True)\n",
    "                print(f\"  \u2713 Copied directory: {item.name}\")\n",
    "    \n",
    "    # Check what we have now\n",
    "    if (results_dir / \"checkpoint_latest.pth\").exists() or (results_dir / \"checkpoint_final.pth\").exists():\n",
    "        print(f\"\\n\u2713 Checkpoint restoration successful.\")\n",
    "    else:\n",
    "        print(\"\\n\u26a0 Warning: No valid checkpoint file (latest/final) found in destination!\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\n\u2717 Checkpoint path not found: {CHECKPOINT_PATH}\")\n",
    "    print(\"Training will start from scratch\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training Execution\n",
    "Launching the nnU-Net training command with the custom trainer and fold configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONTINUING TRAINING WITH CUSTOM OVERSAMPLING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  - Dataset: 002 (BonnFCD_FLAIR)\")\n",
    "print(f\"  - Configuration: 3d_fullres\")\n",
    "print(f\"  - Fold: {FOLD}\")\n",
    "print(f\"  - Trainer: nnUNetTrainerOversampling\")\n",
    "print(f\"  - Rare subjects: {len(RARE_SUBJECTS)}\")\n",
    "print(f\"  - Oversample factor: {OVERSAMPLE_FACTOR}x\")\n",
    "print(f\"  - Continuing from previous checkpoint\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Continue training - nnUNet will automatically detect and load the latest checkpoint\n",
    "!nnUNetv2_train 002 3d_fullres {FOLD} -tr nnUNetTrainerOversampling -p nnUNetPlans --npz --c"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8690716,
     "sourceId": 13668479,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9098051,
     "sourceId": 14258345,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
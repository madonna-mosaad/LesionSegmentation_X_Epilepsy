{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Environment Setup\n",
    "Installing necessary dependencies and importing libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-21T22:03:22.109161Z",
     "iopub.status.busy": "2025-12-21T22:03:22.108913Z",
     "iopub.status.idle": "2025-12-21T22:04:51.377331Z",
     "shell.execute_reply": "2025-12-21T22:04:51.376558Z",
     "shell.execute_reply.started": "2025-12-21T22:03:22.109133Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install nnunetv2\n",
    "\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "print(\"\u2713 Dependencies installed and imported\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Experiment Configuration\n",
    "Defining experiment parameters, selecting the specific **Cross-Validation Fold**, and configuring rare subject oversampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T22:04:59.988720Z",
     "iopub.status.busy": "2025-12-21T22:04:59.988409Z",
     "iopub.status.idle": "2025-12-21T22:04:59.995278Z",
     "shell.execute_reply": "2025-12-21T22:04:59.994612Z",
     "shell.execute_reply.started": "2025-12-21T22:04:59.988659Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training configuration - Fold Selection\n",
    "FOLD = 4  # Select fold (0, 1, 2, 3, 4)\n",
    "\n",
    "TRAINING_TIME_MINUTES = (11 * 60)+ (45)  # 11 hours 45 minutes\n",
    "\n",
    "\n",
    "# Define your dataset paths - MODIFY THESE TO MATCH YOUR KAGGLE PATHS\n",
    "PREPROCESSED_PATH = \"/kaggle/input/preprocessed-bonnfcd-flair/nnUNet_preprocessed/Dataset002_BonnFCD_FLAIR\"\n",
    "RAW_PATH = \"/kaggle/input/preprocessed-bonnfcd-flair/nnUNet_raw_data_base/nnUNet_raw/Dataset002_BonnFCD_FLAIR\"\n",
    "\n",
    "# nnUNet environment variables\n",
    "os.environ['nnUNet_raw'] = \"/kaggle/input/preprocessed-bonnfcd-flair/nnUNet_raw_data_base/nnUNet_raw\"\n",
    "os.environ['nnUNet_preprocessed'] = \"/kaggle/input/preprocessed-bonnfcd-flair/nnUNet_preprocessed\"\n",
    "os.environ['nnUNet_results'] = \"/kaggle/working/nnUNet_results\"\n",
    "\n",
    "# Subjects to oversample (rare abnormalities - non-cortical thickening)\n",
    "RARE_SUBJECTS = [\n",
    "    'sub-00001', 'sub-00003', 'sub-00014', 'sub-00015', 'sub-00016', 'sub-00018', \n",
    "    'sub-00024', 'sub-00027', 'sub-00033', 'sub-00038', 'sub-00040', 'sub-00044', \n",
    "    'sub-00050', 'sub-00053', 'sub-00055', 'sub-00058', 'sub-00060', 'sub-00063', \n",
    "    'sub-00065', 'sub-00073', 'sub-00077', 'sub-00078', 'sub-00080', 'sub-00081', \n",
    "    'sub-00083', 'sub-00087', 'sub-00089', 'sub-00097', 'sub-00098', 'sub-00101', \n",
    "    'sub-00105', 'sub-00109', 'sub-00112', 'sub-00115', 'sub-00116', 'sub-00122', \n",
    "    'sub-00123', 'sub-00126', 'sub-00130', 'sub-00132', 'sub-00133', 'sub-00138', \n",
    "    'sub-00146'\n",
    "]\n",
    "\n",
    "# Oversampling factor for rare subjects\n",
    "OVERSAMPLE_FACTOR = 3.0  # Adjust this value as needed (2.0-5.0 recommended)\n",
    "\n",
    "print(f\"\u2713 Configuration set\")\n",
    "print(f\"  - Number of rare subjects: {len(RARE_SUBJECTS)}\")\n",
    "print(f\"  - Oversample factor: {OVERSAMPLE_FACTOR}x\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Custom nnU-Net Components\n",
    "Defining and generating the `CustomOversamplingDataLoader` and `nnUNetTrainerOversampling` to handle class imbalance during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T22:13:36.080067Z",
     "iopub.status.busy": "2025-12-21T22:13:36.079389Z",
     "iopub.status.idle": "2025-12-21T22:13:36.086407Z",
     "shell.execute_reply": "2025-12-21T22:13:36.085854Z",
     "shell.execute_reply.started": "2025-12-21T22:13:36.080035Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_custom_dataloader_file():\n",
    "    \"\"\"\n",
    "    Create a custom DataLoader that modifies sampling probabilities\n",
    "    \"\"\"\n",
    "    custom_loader_code = '''\n",
    "from nnunetv2.training.dataloading.data_loader import nnUNetDataLoader\n",
    "import numpy as np\n",
    "\n",
    "class CustomOversamplingDataLoader(nnUNetDataLoader):\n",
    "    \"\"\"Custom DataLoader with oversampling for both 2D and 3D\"\"\"\n",
    "    \n",
    "    def __init__(self, data, batch_size, patch_size, final_patch_size=None,\n",
    "                 label_manager=None, oversample_foreground_percent=0.33,\n",
    "                 sampling_probabilities=None, pad_kwargs_data=None,\n",
    "                 pad_mode=\"constant\", rare_subjects=None, oversample_factor=3.0):\n",
    "        \n",
    "        self.rare_subjects = rare_subjects if rare_subjects else []\n",
    "        self.oversample_factor = oversample_factor\n",
    "        \n",
    "        # Initialize parent class WITHOUT num_threads_in_multithreaded\n",
    "        super().__init__(data, batch_size, patch_size, final_patch_size,\n",
    "                        label_manager, oversample_foreground_percent,\n",
    "                        sampling_probabilities, pad_kwargs_data, pad_mode)\n",
    "        \n",
    "        # Modify sampling probabilities after initialization\n",
    "        if self.rare_subjects:\n",
    "            self._modify_sampling_probabilities()\n",
    "    \n",
    "    def _modify_sampling_probabilities(self):\n",
    "        \"\"\"Modify sampling probabilities to favor rare subjects\"\"\"\n",
    "        if not hasattr(self, '_data') or self._data is None:\n",
    "            return\n",
    "        \n",
    "        # Get the list of case identifiers from the dataset\n",
    "        # nnUNetDatasetBlosc2 uses the 'identifiers' attribute\n",
    "        case_ids = None\n",
    "        \n",
    "        # Try the identifiers attribute (works for nnUNetDatasetBlosc2)\n",
    "        if hasattr(self._data, 'identifiers'):\n",
    "            case_ids = self._data.identifiers\n",
    "        \n",
    "        # Fallback: try other possible attributes\n",
    "        if case_ids is None and hasattr(self._data, 'indices'):\n",
    "            case_ids = self._data.indices\n",
    "        \n",
    "        if case_ids is None and hasattr(self._data, 'keys') and callable(self._data.keys):\n",
    "            try:\n",
    "                case_ids = list(self._data.keys())\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if case_ids is None:\n",
    "            print(\"  \u26a0 Warning: Could not extract case IDs from dataset, oversampling disabled\")\n",
    "            return\n",
    "        \n",
    "        num_cases = len(case_ids)\n",
    "        sampling_probs = np.ones(num_cases)\n",
    "        \n",
    "        # Increase weight for rare subjects\n",
    "        rare_count = 0\n",
    "        for idx, case_id in enumerate(case_ids):\n",
    "            # Extract subject ID (format: sub-00001 or sub-00001_0000)\n",
    "            subject_id = str(case_id).split('_')[0]\n",
    "            \n",
    "            if subject_id in self.rare_subjects:\n",
    "                sampling_probs[idx] *= self.oversample_factor\n",
    "                rare_count += 1\n",
    "        \n",
    "        # Normalize probabilities\n",
    "        sampling_probs = sampling_probs / sampling_probs.sum()\n",
    "        self.sampling_probabilities = sampling_probs\n",
    "        \n",
    "        print(f\"  \u2713 Modified sampling: {rare_count}/{num_cases} cases are rare subjects ({self.oversample_factor}x weight)\")\n",
    "'''\n",
    "    \n",
    "    # Save the custom dataloader\n",
    "    os.makedirs(\"/kaggle/working/custom_nnunet\", exist_ok=True)\n",
    "    with open(\"/kaggle/working/custom_nnunet/custom_dataloader.py\", \"w\") as f:\n",
    "        f.write(custom_loader_code)\n",
    "    \n",
    "    print(\"\u2713 Custom DataLoader file created\")\n",
    "\n",
    "create_custom_dataloader_file()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T22:19:23.582713Z",
     "iopub.status.busy": "2025-12-21T22:19:23.582386Z",
     "iopub.status.idle": "2025-12-21T22:19:23.590579Z",
     "shell.execute_reply": "2025-12-21T22:19:23.590013Z",
     "shell.execute_reply.started": "2025-12-21T22:19:23.582664Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from batchgenerators.utilities.file_and_folder_operations import join\n",
    "\n",
    "def create_custom_trainer():\n",
    "    \"\"\"\n",
    "    Create a custom trainer that:\n",
    "    1. Uses oversampling\n",
    "    2. Saves checkpoints more frequently\n",
    "    3. Stops automatically after a fixed time limit (Kaggle-safe)\n",
    "    4. FIX: Removed 'unpack_dataset' entirely to prevent KeyError in nnU-Net v2.6+\n",
    "    \"\"\"\n",
    "\n",
    "    trainer_code = f'''import sys\n",
    "import time\n",
    "from batchgenerators.utilities.file_and_folder_operations import join\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, \"/kaggle/working/custom_nnunet\")\n",
    "\n",
    "from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer\n",
    "from custom_dataloader import CustomOversamplingDataLoader\n",
    "from nnunetv2.training.dataloading.data_loader import nnUNetDataLoader\n",
    "\n",
    "\n",
    "class nnUNetTrainerOversampling(nnUNetTrainer):\n",
    "    \"\"\"\n",
    "    Custom nnU-Net trainer with:\n",
    "    - Oversampling for rare subjects\n",
    "    - Frequent checkpoint saving\n",
    "    - Time-limit safety for Kaggle\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 1. Configuration\n",
    "    # -------------------------------------------------------\n",
    "    rare_subjects = {RARE_SUBJECTS}\n",
    "    oversample_factor = {OVERSAMPLE_FACTOR}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        plans: dict,\n",
    "        configuration: str,\n",
    "        fold: int,\n",
    "        dataset_json: dict,\n",
    "        # unpack_dataset REMOVED entirely\n",
    "        device: torch.device = torch.device('cuda'),\n",
    "    ):\n",
    "        # Initialize parent without unpack_dataset\n",
    "        super().__init__(plans, configuration, fold, dataset_json, device=device)\n",
    "\n",
    "        # SAVE FREQUENCY: save permanent checkpoint every 20 epochs\n",
    "        self.save_every = 20\n",
    "\n",
    "        # TIME LIMIT: stop training after ~11h (Kaggle max is ~12h)\n",
    "        self.max_time_seconds = {TRAINING_TIME_MINUTES} * 60\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 2. Oversampling Logic\n",
    "    # -------------------------------------------------------\n",
    "    def get_tr_and_val_datasets(self):\n",
    "        dataset_tr, dataset_val = super().get_tr_and_val_datasets()\n",
    "        return dataset_tr, dataset_val\n",
    "\n",
    "    def get_plain_dataloaders(self):\n",
    "        dataset_tr, dataset_val = self.get_tr_and_val_datasets()\n",
    "\n",
    "        dl_tr = CustomOversamplingDataLoader(\n",
    "            dataset_tr,\n",
    "            self.batch_size,\n",
    "            self.patch_size,\n",
    "            self.patch_size,\n",
    "            self.label_manager,\n",
    "            oversample_foreground_percent=self.oversample_foreground_percent,\n",
    "            rare_subjects=self.rare_subjects,\n",
    "            oversample_factor=self.oversample_factor,\n",
    "        )\n",
    "\n",
    "        dl_val = nnUNetDataLoader(\n",
    "            dataset_val,\n",
    "            self.batch_size,\n",
    "            self.patch_size,\n",
    "            self.patch_size,\n",
    "            self.label_manager,\n",
    "            oversample_foreground_percent=self.oversample_foreground_percent,\n",
    "        )\n",
    "\n",
    "        return dl_tr, dl_val\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 3. Time-Limit Safety\n",
    "    # -------------------------------------------------------\n",
    "    def on_epoch_end(self):\n",
    "        super().on_epoch_end()\n",
    "\n",
    "        elapsed = time.time() - self.start_time\n",
    "\n",
    "        if elapsed > self.max_time_seconds:\n",
    "            self.print_to_log_file(\n",
    "                f\"\\\\\\\\nTIME LIMIT REACHED ({{elapsed / 3600:.2f}} hours).\"\n",
    "            )\n",
    "            self.print_to_log_file(\n",
    "                \"Stopping training gracefully to save checkpoints.\"\n",
    "            )\n",
    "\n",
    "            # Explicitly save latest checkpoint\n",
    "            self.save_checkpoint(\n",
    "                join(self.output_folder, \"checkpoint_latest.pth\")\n",
    "            )\n",
    "\n",
    "            # Clean shutdown so Kaggle persists outputs\n",
    "            self.on_train_end()\n",
    "            sys.exit(0)\n",
    "'''\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # Save trainer file\n",
    "    # -------------------------------------------------------\n",
    "    import nnunetv2\n",
    "    from pathlib import Path\n",
    "\n",
    "    nnunet_trainers_dir = (\n",
    "        Path(nnunetv2.__file__).parent / \"training\" / \"nnUNetTrainer\"\n",
    "    )\n",
    "    trainer_file = nnunet_trainers_dir / \"nnUNetTrainerOversampling.py\"\n",
    "\n",
    "    with open(trainer_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(trainer_code)\n",
    "\n",
    "    print(\"\u2714\ufe0f Custom Trainer updated (unpack_dataset removed completely)\")\n",
    "\n",
    "\n",
    "create_custom_trainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Preparation\n",
    "Verifying the input dataset integrity and copying it to a writable directory (`/kaggle/working`) for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T22:05:15.509580Z",
     "iopub.status.busy": "2025-12-21T22:05:15.509298Z",
     "iopub.status.idle": "2025-12-21T22:05:15.889576Z",
     "shell.execute_reply": "2025-12-21T22:05:15.889018Z",
     "shell.execute_reply.started": "2025-12-21T22:05:15.509559Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def verify_dataset():\n",
    "    \"\"\"Verify the preprocessed dataset structure and subject identifiers\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DATASET VERIFICATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Check preprocessed data\n",
    "    if os.path.exists(PREPROCESSED_PATH):\n",
    "        print(f\"\\n\u2713 Preprocessed data found at: {PREPROCESSED_PATH}\")\n",
    "        \n",
    "        # List nnUNet plans\n",
    "        plans_files = list(Path(PREPROCESSED_PATH).glob(\"nnUNetPlans*.json\"))\n",
    "        if plans_files:\n",
    "            print(f\"  - Found {len(plans_files)} plan file(s)\")\n",
    "            for plan in plans_files:\n",
    "                print(f\"    \u2022 {plan.name}\")\n",
    "        \n",
    "        # Check for preprocessed data folders\n",
    "        print(\"\\n\u2713 Checking preprocessed data structure:\")\n",
    "        for config in ['nnUNetPlans_2d', 'nnUNetPlans_3d_fullres']:\n",
    "            config_path = Path(PREPROCESSED_PATH) / config\n",
    "            if config_path.exists():\n",
    "                # Count .npz or .npy files in the configuration folder\n",
    "                npz_files = list(config_path.glob(\"**/*.npz\"))\n",
    "                npy_files = list(config_path.glob(\"**/*.npy\"))\n",
    "                pkl_files = list(config_path.glob(\"**/*.pkl\"))\n",
    "                \n",
    "                total_files = len(npz_files) + len(npy_files) + len(pkl_files)\n",
    "                print(f\"  - {config}: {total_files} preprocessed files\")\n",
    "                \n",
    "                # Try to extract subject IDs from filenames\n",
    "                if npz_files or npy_files or pkl_files:\n",
    "                    all_files = npz_files + npy_files + pkl_files\n",
    "                    subjects = set()\n",
    "                    for f in all_files[:50]:  # Sample first 50 files\n",
    "                        # Extract subject ID (format might be: sub-00001.npz or sub-00001_0000.npz)\n",
    "                        fname = f.stem\n",
    "                        if fname.startswith('sub-'):\n",
    "                            subject_id = fname.split('_')[0]\n",
    "                            subjects.add(subject_id)\n",
    "                    \n",
    "                    if subjects:\n",
    "                        print(f\"    \u2022 Found {len(subjects)} unique subjects (sampled)\")\n",
    "                        \n",
    "                        # Check rare subjects\n",
    "                        rare_found = [s for s in RARE_SUBJECTS if s in subjects]\n",
    "                        if rare_found:\n",
    "                            print(f\"    \u2022 {len(rare_found)} rare subjects found in sample\")\n",
    "    else:\n",
    "        print(f\"\\n\u2717 Preprocessed data NOT found at: {PREPROCESSED_PATH}\")\n",
    "        return False\n",
    "    \n",
    "    # Also check dataset.json for complete subject list\n",
    "    dataset_json_path = Path(PREPROCESSED_PATH) / \"dataset.json\"\n",
    "    if dataset_json_path.exists():\n",
    "        print(\"\\n\u2713 Reading dataset.json for complete subject list...\")\n",
    "        with open(dataset_json_path, 'r') as f:\n",
    "            dataset_info = json.load(f)\n",
    "            if 'training' in dataset_info:\n",
    "                training_cases = dataset_info['training']\n",
    "                subjects = set()\n",
    "                for case in training_cases:\n",
    "                    # Extract subject ID from image path\n",
    "                    img_path = case.get('image', '')\n",
    "                    if isinstance(img_path, list):\n",
    "                        img_path = img_path[0]\n",
    "                    fname = Path(img_path).stem\n",
    "                    if fname.startswith('sub-'):\n",
    "                        subject_id = fname.split('_')[0]\n",
    "                        subjects.add(subject_id)\n",
    "                \n",
    "                print(f\"  - Total training subjects: {len(subjects)}\")\n",
    "                \n",
    "                # Check rare subjects\n",
    "                rare_found = [s for s in RARE_SUBJECTS if s in subjects]\n",
    "                print(f\"  - Rare subjects found: {len(rare_found)}/{len(RARE_SUBJECTS)}\")\n",
    "                \n",
    "                if len(rare_found) < len(RARE_SUBJECTS):\n",
    "                    missing = set(RARE_SUBJECTS) - subjects\n",
    "                    print(f\"\\n\u26a0 Warning: {len(missing)} rare subjects not in dataset:\")\n",
    "                    for m in sorted(missing)[:10]:\n",
    "                        print(f\"    \u2022 {m}\")\n",
    "                    if len(missing) > 10:\n",
    "                        print(f\"    ... and {len(missing)-10} more\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    return True\n",
    "\n",
    "verify_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T22:05:21.921519Z",
     "iopub.status.busy": "2025-12-21T22:05:21.921254Z",
     "iopub.status.idle": "2025-12-21T22:05:44.573813Z",
     "shell.execute_reply": "2025-12-21T22:05:44.572984Z",
     "shell.execute_reply.started": "2025-12-21T22:05:21.921498Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Copy preprocessed data to writable location\n",
    "source = \"/kaggle/input/preprocessed-bonnfcd-flair/nnUNet_preprocessed/Dataset002_BonnFCD_FLAIR\"\n",
    "dest = \"/kaggle/working/nnUNet_preprocessed/Dataset002_BonnFCD_FLAIR\"\n",
    "\n",
    "print(\"Copying preprocessed data to writable location...\")\n",
    "print(f\"From: {source}\")\n",
    "print(f\"To: {dest}\")\n",
    "\n",
    "# Create destination directory\n",
    "Path(dest).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy the entire dataset\n",
    "shutil.copytree(source, dest, dirs_exist_ok=True)\n",
    "\n",
    "print(f\"\u2713 Copied successfully\")\n",
    "\n",
    "# Update environment variable\n",
    "os.environ['nnUNet_preprocessed'] = \"/kaggle/working/nnUNet_preprocessed\"\n",
    "\n",
    "print(f\"\u2713 Updated nnUNet_preprocessed path to: {os.environ['nnUNet_preprocessed']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "\n",
    "plans_path = \"/kaggle/working/nnUNet_preprocessed/Dataset002_BonnFCD_FLAIR/nnUNetPlans.json\"\n",
    "\n",
    "# Load plans\n",
    "with open(plans_path, \"r\") as f:\n",
    "    plans = json.load(f)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Ensure the augmentation keys exist\n",
    "# ----------------------------------------------------\n",
    "if \"data_augmentation\" not in plans:\n",
    "    plans[\"data_augmentation\"] = {}\n",
    "\n",
    "if \"spatial\" not in plans[\"data_augmentation\"]:\n",
    "    plans[\"data_augmentation\"][\"spatial\"] = {}\n",
    "\n",
    "if \"intensity\" not in plans[\"data_augmentation\"]:\n",
    "    plans[\"data_augmentation\"][\"intensity\"] = {}\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# \ud83d\udd25 Doubled spatial augmentations\n",
    "# ----------------------------------------------------\n",
    "# Rotation: \u00b160 degrees (Default was 30)\n",
    "plans[\"data_augmentation\"][\"spatial\"][\"rotation\"] = {\n",
    "    \"x\": 60 * (math.pi / 180),\n",
    "    \"y\": 60 * (math.pi / 180),\n",
    "    \"z\": 60 * (math.pi / 180)\n",
    "}\n",
    "\n",
    "# Scale: Range [0.70, 1.50] (Default was [0.85, 1.25])\n",
    "# Logic: Doubled the deviation from 1.0 (15%->30% down, 25%->50% up)\n",
    "plans[\"data_augmentation\"][\"spatial\"][\"scale_range\"] = [0.70, 1.50]\n",
    "\n",
    "# Elastic Deformation: Still Disabled (Doubling None is None)\n",
    "# If you want to force it on, you must add a dictionary here.\n",
    "plans[\"data_augmentation\"][\"spatial\"][\"elastic_deform\"] = None\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# \ud83d\udd25 Doubled intensity augmentations\n",
    "# ----------------------------------------------------\n",
    "# Brightness: Range [0.5, 1.5] (Default was [0.75, 1.25])\n",
    "plans[\"data_augmentation\"][\"intensity\"][\"brightness\"] = [0.5, 1.5]\n",
    "\n",
    "# Contrast: Range [0.5, 1.5] (Default was [0.75, 1.25])\n",
    "plans[\"data_augmentation\"][\"intensity\"][\"contrast\"] = [0.5, 1.5]\n",
    "\n",
    "# Gaussian Noise: Variance 0.2 (Default was 0.1)\n",
    "plans[\"data_augmentation\"][\"intensity\"][\"gaussian_noise_std\"] = 0.2\n",
    "\n",
    "# Gamma: Range [0.4, 2.0] (Default was [0.7, 1.5])\n",
    "# Logic: Doubled the deviation from 1.0\n",
    "plans[\"data_augmentation\"][\"intensity\"][\"gamma_range\"] = [0.4, 2.0]\n",
    "\n",
    "# Save back\n",
    "with open(plans_path, \"w\") as f:\n",
    "    json.dump(plans, f, indent=4)\n",
    "\n",
    "print(\"\ud83d\udd25 Augmentation parameters DOUBLED successfully!\")\n",
    "print(\"- Rotation: \u00b160\u00b0\")\n",
    "print(\"- Scale: 0.70 - 1.50\")\n",
    "print(\"- Noise/Contrast/Gamma: Deviation doubled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# \ud83e\uddea Custom 5-Fold Cross-Validation Splits\n",
    "# ----------------------------------------------------\n",
    "# Hardcoded splits to ensure consistency across Kaggle sessions\n",
    "custom_splits = [\n",
    "    {\n",
    "        \"train\": [\n",
    "            \"sub-00001\",\n",
    "            \"sub-00003\",\n",
    "            \"sub-00010\",\n",
    "            \"sub-00014\",\n",
    "            \"sub-00015\",\n",
    "            \"sub-00016\",\n",
    "            \"sub-00024\",\n",
    "            \"sub-00027\",\n",
    "            \"sub-00033\",\n",
    "            \"sub-00038\",\n",
    "            \"sub-00040\",\n",
    "            \"sub-00044\",\n",
    "            \"sub-00047\",\n",
    "            \"sub-00050\",\n",
    "            \"sub-00053\",\n",
    "            \"sub-00055\",\n",
    "            \"sub-00058\",\n",
    "            \"sub-00059\",\n",
    "            \"sub-00060\",\n",
    "            \"sub-00063\",\n",
    "            \"sub-00071\",\n",
    "            \"sub-00072\",\n",
    "            \"sub-00073\",\n",
    "            \"sub-00076\",\n",
    "            \"sub-00077\",\n",
    "            \"sub-00078\",\n",
    "            \"sub-00080\",\n",
    "            \"sub-00083\",\n",
    "            \"sub-00089\",\n",
    "            \"sub-00090\",\n",
    "            \"sub-00091\",\n",
    "            \"sub-00097\",\n",
    "            \"sub-00098\",\n",
    "            \"sub-00101\",\n",
    "            \"sub-00112\",\n",
    "            \"sub-00120\",\n",
    "            \"sub-00122\",\n",
    "            \"sub-00123\",\n",
    "            \"sub-00130\",\n",
    "            \"sub-00132\",\n",
    "            \"sub-00133\",\n",
    "            \"sub-00139\",\n",
    "            \"sub-00140\",\n",
    "            \"sub-00141\",\n",
    "            \"sub-00146\"\n",
    "        ],\n",
    "        \"val\": [\n",
    "            \"sub-00018\",\n",
    "            \"sub-00065\",\n",
    "            \"sub-00068\",\n",
    "            \"sub-00081\",\n",
    "            \"sub-00087\",\n",
    "            \"sub-00105\",\n",
    "            \"sub-00109\",\n",
    "            \"sub-00115\",\n",
    "            \"sub-00116\",\n",
    "            \"sub-00126\",\n",
    "            \"sub-00131\",\n",
    "            \"sub-00138\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"train\": [\n",
    "            \"sub-00001\",\n",
    "            \"sub-00003\",\n",
    "            \"sub-00014\",\n",
    "            \"sub-00015\",\n",
    "            \"sub-00016\",\n",
    "            \"sub-00018\",\n",
    "            \"sub-00024\",\n",
    "            \"sub-00033\",\n",
    "            \"sub-00038\",\n",
    "            \"sub-00040\",\n",
    "            \"sub-00047\",\n",
    "            \"sub-00050\",\n",
    "            \"sub-00053\",\n",
    "            \"sub-00055\",\n",
    "            \"sub-00058\",\n",
    "            \"sub-00059\",\n",
    "            \"sub-00065\",\n",
    "            \"sub-00068\",\n",
    "            \"sub-00071\",\n",
    "            \"sub-00077\",\n",
    "            \"sub-00078\",\n",
    "            \"sub-00080\",\n",
    "            \"sub-00081\",\n",
    "            \"sub-00083\",\n",
    "            \"sub-00087\",\n",
    "            \"sub-00089\",\n",
    "            \"sub-00090\",\n",
    "            \"sub-00091\",\n",
    "            \"sub-00097\",\n",
    "            \"sub-00098\",\n",
    "            \"sub-00105\",\n",
    "            \"sub-00109\",\n",
    "            \"sub-00112\",\n",
    "            \"sub-00115\",\n",
    "            \"sub-00116\",\n",
    "            \"sub-00120\",\n",
    "            \"sub-00126\",\n",
    "            \"sub-00130\",\n",
    "            \"sub-00131\",\n",
    "            \"sub-00133\",\n",
    "            \"sub-00138\",\n",
    "            \"sub-00139\",\n",
    "            \"sub-00140\",\n",
    "            \"sub-00141\",\n",
    "            \"sub-00146\"\n",
    "        ],\n",
    "        \"val\": [\n",
    "            \"sub-00010\",\n",
    "            \"sub-00027\",\n",
    "            \"sub-00044\",\n",
    "            \"sub-00060\",\n",
    "            \"sub-00063\",\n",
    "            \"sub-00072\",\n",
    "            \"sub-00073\",\n",
    "            \"sub-00076\",\n",
    "            \"sub-00101\",\n",
    "            \"sub-00122\",\n",
    "            \"sub-00123\",\n",
    "            \"sub-00132\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"train\": [\n",
    "            \"sub-00003\",\n",
    "            \"sub-00010\",\n",
    "            \"sub-00018\",\n",
    "            \"sub-00024\",\n",
    "            \"sub-00027\",\n",
    "            \"sub-00033\",\n",
    "            \"sub-00038\",\n",
    "            \"sub-00040\",\n",
    "            \"sub-00044\",\n",
    "            \"sub-00047\",\n",
    "            \"sub-00050\",\n",
    "            \"sub-00055\",\n",
    "            \"sub-00058\",\n",
    "            \"sub-00059\",\n",
    "            \"sub-00060\",\n",
    "            \"sub-00063\",\n",
    "            \"sub-00065\",\n",
    "            \"sub-00068\",\n",
    "            \"sub-00071\",\n",
    "            \"sub-00072\",\n",
    "            \"sub-00073\",\n",
    "            \"sub-00076\",\n",
    "            \"sub-00077\",\n",
    "            \"sub-00080\",\n",
    "            \"sub-00081\",\n",
    "            \"sub-00087\",\n",
    "            \"sub-00090\",\n",
    "            \"sub-00097\",\n",
    "            \"sub-00098\",\n",
    "            \"sub-00101\",\n",
    "            \"sub-00105\",\n",
    "            \"sub-00109\",\n",
    "            \"sub-00112\",\n",
    "            \"sub-00115\",\n",
    "            \"sub-00116\",\n",
    "            \"sub-00122\",\n",
    "            \"sub-00123\",\n",
    "            \"sub-00126\",\n",
    "            \"sub-00131\",\n",
    "            \"sub-00132\",\n",
    "            \"sub-00133\",\n",
    "            \"sub-00138\",\n",
    "            \"sub-00139\",\n",
    "            \"sub-00140\",\n",
    "            \"sub-00141\",\n",
    "            \"sub-00146\"\n",
    "        ],\n",
    "        \"val\": [\n",
    "            \"sub-00001\",\n",
    "            \"sub-00014\",\n",
    "            \"sub-00015\",\n",
    "            \"sub-00016\",\n",
    "            \"sub-00053\",\n",
    "            \"sub-00078\",\n",
    "            \"sub-00083\",\n",
    "            \"sub-00089\",\n",
    "            \"sub-00091\",\n",
    "            \"sub-00120\",\n",
    "            \"sub-00130\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"train\": [\n",
    "            \"sub-00001\",\n",
    "            \"sub-00003\",\n",
    "            \"sub-00010\",\n",
    "            \"sub-00014\",\n",
    "            \"sub-00015\",\n",
    "            \"sub-00016\",\n",
    "            \"sub-00018\",\n",
    "            \"sub-00027\",\n",
    "            \"sub-00044\",\n",
    "            \"sub-00050\",\n",
    "            \"sub-00053\",\n",
    "            \"sub-00060\",\n",
    "            \"sub-00063\",\n",
    "            \"sub-00065\",\n",
    "            \"sub-00068\",\n",
    "            \"sub-00072\",\n",
    "            \"sub-00073\",\n",
    "            \"sub-00076\",\n",
    "            \"sub-00078\",\n",
    "            \"sub-00080\",\n",
    "            \"sub-00081\",\n",
    "            \"sub-00083\",\n",
    "            \"sub-00087\",\n",
    "            \"sub-00089\",\n",
    "            \"sub-00090\",\n",
    "            \"sub-00091\",\n",
    "            \"sub-00097\",\n",
    "            \"sub-00098\",\n",
    "            \"sub-00101\",\n",
    "            \"sub-00105\",\n",
    "            \"sub-00109\",\n",
    "            \"sub-00112\",\n",
    "            \"sub-00115\",\n",
    "            \"sub-00116\",\n",
    "            \"sub-00120\",\n",
    "            \"sub-00122\",\n",
    "            \"sub-00123\",\n",
    "            \"sub-00126\",\n",
    "            \"sub-00130\",\n",
    "            \"sub-00131\",\n",
    "            \"sub-00132\",\n",
    "            \"sub-00138\",\n",
    "            \"sub-00139\",\n",
    "            \"sub-00140\",\n",
    "            \"sub-00141\",\n",
    "            \"sub-00146\"\n",
    "        ],\n",
    "        \"val\": [\n",
    "            \"sub-00024\",\n",
    "            \"sub-00033\",\n",
    "            \"sub-00038\",\n",
    "            \"sub-00040\",\n",
    "            \"sub-00047\",\n",
    "            \"sub-00055\",\n",
    "            \"sub-00058\",\n",
    "            \"sub-00059\",\n",
    "            \"sub-00071\",\n",
    "            \"sub-00077\",\n",
    "            \"sub-00133\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"train\": [\n",
    "            \"sub-00001\",\n",
    "            \"sub-00010\",\n",
    "            \"sub-00014\",\n",
    "            \"sub-00015\",\n",
    "            \"sub-00016\",\n",
    "            \"sub-00018\",\n",
    "            \"sub-00024\",\n",
    "            \"sub-00027\",\n",
    "            \"sub-00033\",\n",
    "            \"sub-00038\",\n",
    "            \"sub-00040\",\n",
    "            \"sub-00044\",\n",
    "            \"sub-00047\",\n",
    "            \"sub-00053\",\n",
    "            \"sub-00055\",\n",
    "            \"sub-00058\",\n",
    "            \"sub-00059\",\n",
    "            \"sub-00060\",\n",
    "            \"sub-00063\",\n",
    "            \"sub-00065\",\n",
    "            \"sub-00068\",\n",
    "            \"sub-00071\",\n",
    "            \"sub-00072\",\n",
    "            \"sub-00073\",\n",
    "            \"sub-00076\",\n",
    "            \"sub-00077\",\n",
    "            \"sub-00078\",\n",
    "            \"sub-00081\",\n",
    "            \"sub-00083\",\n",
    "            \"sub-00087\",\n",
    "            \"sub-00089\",\n",
    "            \"sub-00091\",\n",
    "            \"sub-00101\",\n",
    "            \"sub-00105\",\n",
    "            \"sub-00109\",\n",
    "            \"sub-00115\",\n",
    "            \"sub-00116\",\n",
    "            \"sub-00120\",\n",
    "            \"sub-00122\",\n",
    "            \"sub-00123\",\n",
    "            \"sub-00126\",\n",
    "            \"sub-00130\",\n",
    "            \"sub-00131\",\n",
    "            \"sub-00132\",\n",
    "            \"sub-00133\",\n",
    "            \"sub-00138\"\n",
    "        ],\n",
    "        \"val\": [\n",
    "            \"sub-00003\",\n",
    "            \"sub-00050\",\n",
    "            \"sub-00080\",\n",
    "            \"sub-00090\",\n",
    "            \"sub-00097\",\n",
    "            \"sub-00098\",\n",
    "            \"sub-00112\",\n",
    "            \"sub-00139\",\n",
    "            \"sub-00140\",\n",
    "            \"sub-00141\",\n",
    "            \"sub-00146\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define the target path where nnU-Net expects the splits file\n",
    "# It must be in the preprocessed directory of the specific task\n",
    "preprocessed_dir = \"/kaggle/working/nnUNet_preprocessed/Dataset002_BonnFCD_FLAIR\"\n",
    "splits_file = os.path.join(preprocessed_dir, \"splits_final.json\")\n",
    "\n",
    "# Write the splits to the file\n",
    "os.makedirs(preprocessed_dir, exist_ok=True)\n",
    "with open(splits_file, \"w\") as f:\n",
    "    json.dump(custom_splits, f, indent=4)\n",
    "\n",
    "print(f\"\u2705 Custom splits enforced successfully!\")\n",
    "print(f\"  - Location: {splits_file}\")\n",
    "print(f\"  - Total folds: {len(custom_splits)}\")\n",
    "print(f\"  - nnU-Net will now use these fixed splits instead of random ones.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T22:05:48.790042Z",
     "iopub.status.busy": "2025-12-21T22:05:48.789677Z",
     "iopub.status.idle": "2025-12-21T22:05:48.794486Z",
     "shell.execute_reply": "2025-12-21T22:05:48.793669Z",
     "shell.execute_reply.started": "2025-12-21T22:05:48.790021Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2713 Dataset is already preprocessed - skipping preprocessing step\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nOversampling will be applied during training via custom DataLoader\")\n",
    "print(\"This modifies sampling probabilities AFTER preprocessing as intended\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T22:21:32.156676Z",
     "iopub.status.busy": "2025-12-21T22:21:32.156068Z",
     "iopub.status.idle": "2025-12-21T22:21:32.161372Z",
     "shell.execute_reply": "2025-12-21T22:21:32.160655Z",
     "shell.execute_reply.started": "2025-12-21T22:21:32.156651Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# CELL: Disable torch.compile for P100 GPU compatibility\n",
    "import os\n",
    "os.environ['nnUNet_compile'] = 'false'\n",
    "\n",
    "print(\"\u2713 Disabled torch.compile for GPU compatibility\")\n",
    "print(\"  (P100 GPU doesn't support Triton compiler)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Training Execution\n",
    "Final configuration checks and executing the nnU-Net training command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T22:21:35.292806Z",
     "iopub.status.busy": "2025-12-21T22:21:35.292172Z",
     "iopub.status.idle": "2025-12-21T22:31:37.212312Z",
     "shell.execute_reply": "2025-12-21T22:31:37.211317Z",
     "shell.execute_reply.started": "2025-12-21T22:21:35.292769Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING TRAINING WITH CUSTOM OVERSAMPLING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  - Dataset: 002 (BonnFCD_FLAIR)\")\n",
    "print(f\"  - Configuration: 3d_fullres\")\n",
    "print(f\"  - Fold: {FOLD}\")\n",
    "print(f\"  - Trainer: nnUNetTrainerOversampling\")\n",
    "print(f\"  - Rare subjects: {len(RARE_SUBJECTS)}\")\n",
    "print(f\"  - Oversample factor: {OVERSAMPLE_FACTOR}x\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "!nnUNetv2_train 002 3d_fullres {FOLD} -tr nnUNetTrainerOversampling -p nnUNetPlans --npz"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8690716,
     "sourceId": 13668479,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
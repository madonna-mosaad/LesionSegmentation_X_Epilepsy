{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "225ae3a3",
   "metadata": {},
   "source": [
    "This is a specialized Inference Notebook designed to work with your specific file structure and the Custom Trainer (`nnUNetTrainerOversampling`) you used during training.\n",
    "\n",
    "Since nnU-Net is very strict about folder structures (`nnUNet_results`), this notebook **artificially reconstructs the expected folder hierarchy** so the inference command can find your checkpoint without errors.\n",
    "\n",
    "### **Inference Notebook: BonnFCD Segmentation**\n",
    "\n",
    "#### **Cell 1: Environment Setup**\n",
    "\n",
    "Install nnU-Net, MedPy (for metrics), and import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Add the src directory to sys.path so we can import config\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\", \"src\")))\n",
    "\n",
    "from config import *\n",
    "setup_env()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9ad95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install nnU-Net V2 and MedPy for evaluation\n",
    "# !pip install nnunetv2 medpy pandas\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from medpy.metric.binary import dc, hd95, asd\n",
    "from medpy.metric.binary import precision as prec\n",
    "from medpy.metric.binary import recall as rec\n",
    "\n",
    "print(\"\u2713 Environment ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7c183f",
   "metadata": {},
   "source": [
    "#### **Cell 2: Define Paths & Environment Variables**\n",
    "\n",
    "Here we map the paths based on the file tree you provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cb51c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# NNUNET RESULTS INPUT (User Input)\n",
    "# ---------------------------------------------------------\n",
    "NNUNET_MODEL_OUTPUT_DATA = nnUNet_results\n",
    "\n",
    "FOLD = 4  # Select fold (0, 1, 2, 3, 4)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# CHECKPOINT FILE NAME (User Choice)\n",
    "# ---------------------------------------------------------\n",
    "CHECKPOINT_NAME = \"checkpoint_best.pth\"\n",
    "# or: \"checkpoint_final.pth\"\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# FULL CHECKPOINT PATH (Derived)\n",
    "# ---------------------------------------------------------\n",
    "CHECKPOINT_SOURCE = os.path.join(\n",
    "    NNUNET_MODEL_OUTPUT_DATA,\n",
    "    \"nnUNet_results\",\n",
    "    \"Dataset002_BonnFCD_FLAIR\",\n",
    "    \"nnUNetTrainerOversampling__nnUNetPlans__3d_fullres\",\n",
    "    f\"fold_{FOLD}\",\n",
    "    CHECKPOINT_NAME,\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. DATA PATHS (Based on your provided file tree)\n",
    "# ---------------------------------------------------------\n",
    "# Path where the Test images are located (imagesTs)\n",
    "INPUT_IMAGES_FOLDER = os.path.join(nnUNet_raw, 'Dataset002_BonnFCD_FLAIR', 'imagesTs')\n",
    "\n",
    "# Path to the preprocessed data (needed for plans.json)\n",
    "PREPROCESSED_BASE = nnUNet_preprocessed\n",
    "\n",
    "# Where to save predictions\n",
    "OUTPUT_FOLDER = \"../results/inference_output\"\n \"../data/inference_output\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. NNUNET ENVIRONMENT VARIABLES\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "print(f\"\u2713 Input Folder: {INPUT_IMAGES_FOLDER}\")\n",
    "print(f\"\u2713 Output Folder: {OUTPUT_FOLDER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e3dc97",
   "metadata": {},
   "source": [
    "#### **Cell 2b: Inspect Checkpoint Training History (Optional)**\n",
    "\n",
    "Use this cell to visualize the training history (Loss & Dice) directly from the checkpoint file. This confirms if the model was trained properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67964a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading checkpoint: {CHECKPOINT_SOURCE} ...\")\n",
    "\n",
    "try:\n",
    "    # FIX: Added weights_only=False to bypass the new PyTorch 2.6 security check\n",
    "    checkpoint = torch.load(CHECKPOINT_SOURCE, map_location='cpu', weights_only=False)\n",
    "\n",
    "    # 1. Basic Info (Epoch)\n",
    "    # Note: nnU-Net epochs start at 0, so if it says 999, you finished 1000 epochs.\n",
    "    current_epoch = checkpoint.get('epoch', 'Unknown')\n",
    "    print(f\"\\n\u2705 Training Status:\")\n",
    "    print(f\"  - Last Completed Epoch: {current_epoch + 1 if isinstance(current_epoch, int) else current_epoch}\")\n",
    "\n",
    "    # 2. Extract Logging History\n",
    "    if 'logging' in checkpoint:\n",
    "        log = checkpoint['logging']\n",
    "        \n",
    "        # Extract lists\n",
    "        train_losses = log.get('train_losses', [])\n",
    "        val_losses = log.get('val_losses', [])\n",
    "        mean_dice = log.get('mean_fg_dice', [])  # Pseudo dice on validation set\n",
    "        \n",
    "        if len(train_losses) > 0:\n",
    "            latest_epoch = len(train_losses)  # human-readable epoch number\n",
    "\n",
    "            print(f\"\\n\ud83d\udcca Latest Metrics (Epoch {latest_epoch}):\")\n",
    "            print(f\"  - Training Loss:   {train_losses[-1]:.4f}\")\n",
    "            print(f\"  - Validation Loss: {val_losses[-1]:.4f}\")\n",
    "            print(f\"  - Validation Dice: {mean_dice[-1]:.4f} (Mean Foreground)\")\n",
    "            \n",
    "            # --- Save Training History to CSV ---\n",
    "            # Lengths might differ slightly if training crashed\n",
    "            min_len = min(len(train_losses), len(val_losses), len(mean_dice))\n",
    "            history_data = {\n",
    "                \"Epoch\": list(range(1, min_len + 1)),  # human-readable\n",
    "                \"Train_Loss\": train_losses[:min_len],\n",
    "                \"Val_Loss\": val_losses[:min_len],\n",
    "                \"Val_Dice\": mean_dice[:min_len]\n",
    "            }\n",
    "            df_history = pd.DataFrame(history_data)\n",
    "            \n",
    "            history_csv_path = Path(OUTPUT_FOLDER) / \"training_history.csv\"\n",
    "            df_history.to_csv(history_csv_path, index=False)\n",
    "            print(f\"\u2713 Training history saved to: {history_csv_path}\")\n",
    "\n",
    "            # --- Save Latest Metrics to CSV ---\n",
    "            latest_metrics = {\n",
    "                \"Epoch\": [latest_epoch],\n",
    "                \"Train_Loss\": [train_losses[-1]],\n",
    "                \"Val_Loss\": [val_losses[-1]],\n",
    "                \"Val_Dice\": [mean_dice[-1]]\n",
    "            }\n",
    "            df_latest = pd.DataFrame(latest_metrics)\n",
    "\n",
    "            latest_csv_path = Path(OUTPUT_FOLDER) / \"latest_metrics.csv\"\n",
    "            df_latest.to_csv(latest_csv_path, index=False)\n",
    "            print(f\"\u2713 Latest metrics saved to: {latest_csv_path}\")\n",
    "\n",
    "            # --- Plotting and Saving ---\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            \n",
    "            # Loss Plot\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(train_losses, label='Train Loss', alpha=0.7)\n",
    "            plt.plot(val_losses, label='Val Loss', alpha=0.7)\n",
    "            plt.title(\"Training vs Validation Loss\")\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Dice Plot\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(mean_dice, label='Mean Foreground Dice')\n",
    "            plt.title(\"Validation Dice Score (Pseudo)\")\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(\"Dice Score\")\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save Plot\n",
    "            history_plot_path = Path(OUTPUT_FOLDER) / \"training_history.png\"\n",
    "            plt.savefig(history_plot_path)\n",
    "            print(f\"\u2713 Training history plot saved to: {history_plot_path}\")\n",
    "            \n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"\u26a0 'logging' key found but lists are empty.\")\n",
    "            \n",
    "    else:\n",
    "        print(\"\u26a0 'logging' key not found in checkpoint. Available keys:\", checkpoint.keys())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"\u274c Error: File not found at {CHECKPOINT_SOURCE}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error reading checkpoint: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5782cbe9",
   "metadata": {},
   "source": [
    "#### **Cell 3: Register Custom Trainer (Crucial)**\n",
    "\n",
    "Since you trained with `nnUNetTrainerOversampling`, the inference command needs to know this class exists, even if we are just predicting. We create a dummy file so nnU-Net doesn't crash on import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e82f109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy custom trainer file so nnU-Net can find the class definition\n",
    "custom_dir = \"../data/custom_nnunet\"\n",
    "os.makedirs(custom_dir, exist_ok=True)\n",
    "\n",
    "trainer_code = '''\n",
    "from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer\n",
    "\n",
    "class nnUNetTrainerOversampling(nnUNetTrainer):\n",
    "    \"\"\"\n",
    "    Dummy class for inference. \n",
    "    We don't need the oversampling logic here, just the class existence \n",
    "    so nnU-Net can load the checkpoint structure correctly.\n",
    "    \"\"\"\n",
    "    def __init__(self, plans: dict, configuration: str, fold: int, dataset_json: dict, device=None):\n",
    "        super().__init__(plans, configuration, fold, dataset_json, device=device)\n",
    "'''\n",
    "\n",
    "with open(f\"{custom_dir}/nnUNetTrainerOversampling.py\", \"w\") as f:\n",
    "    f.write(trainer_code)\n",
    "\n",
    "# Add to python path\n",
    "\n",
    "# Move it to where nnU-Net looks for trainers (internal hack)\n",
    "import nnunetv2\n",
    "nnunet_trainers_dir = Path(nnunetv2.__file__).parent / \"training\" / \"nnUNetTrainer\"\n",
    "shutil.copy(f\"{custom_dir}/nnUNetTrainerOversampling.py\", nnunet_trainers_dir / \"nnUNetTrainerOversampling.py\")\n",
    "\n",
    "print(\"\u2713 Custom Trainer registered for Inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedf4e4a",
   "metadata": {},
   "source": [
    "#### **Cell 4: Reconstruct Results Folder Structure**\n",
    "\n",
    "nnU-Net will look for the checkpoint in a specific path format: `DatasetNAME / Trainer__Plans__Config / fold_X`. We manually create this and copy your checkpoint there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d891708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the strict structure nnU-Net expects\n",
    "# Format: DatasetXXX_Name / TrainerName__PlansName__ConfigName / fold_X\n",
    "dataset_name = \"Dataset002_BonnFCD_FLAIR\"\n",
    "trainer_name = \"nnUNetTrainerOversampling\" # MUST match what you used in training\n",
    "plans_name = \"nnUNetPlans\"\n",
    "config_name = \"3d_fullres\"\n",
    "\n",
    "expected_folder = results_base / dataset_name / f\"{trainer_name}__{plans_name}__{config_name}\" / f\"fold_{FOLD}\"\n",
    "expected_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy the checkpoint\n",
    "src_ckpt = Path(CHECKPOINT_SOURCE)\n",
    "dest_ckpt = expected_folder / \"checkpoint_final.pth\"\n",
    "\n",
    "if src_ckpt.exists():\n",
    "    shutil.copy2(src_ckpt, dest_ckpt)\n",
    "    print(f\"\u2713 Checkpoint moved to:\\n  {dest_ckpt}\")\n",
    "    \n",
    "    # Copy metadata (plans.json, dataset.json) from the trained model folder\n",
    "    # These are needed for inference to know how to process data\n",
    "    # expected_folder.parent is .../Trainer__Plans__Config/\n",
    "    \n",
    "    # FIX: Correctly construct the path including 'nnUNet_results'\n",
    "    # AND use the correct variable name defined in Cell 2 (NNUNET_MODEL_OUTPUT_DATA)\n",
    "    trained_model_folder = Path(NNUNET_MODEL_OUTPUT_DATA) / \"nnUNet_results\" / dataset_name / f\"{trainer_name}__{plans_name}__{config_name}\"\n",
    "    \n",
    "    if trained_model_folder.exists():\n",
    "        print(f\"\u2713 Found trained model folder: {trained_model_folder}\")\n",
    "        \n",
    "        # Copy plans.json\n",
    "        plans_src = trained_model_folder / \"plans.json\"\n",
    "        plans_dest = expected_folder.parent / \"plans.json\" \n",
    "        if plans_src.exists():\n",
    "            shutil.copy2(plans_src, plans_dest)\n",
    "            print(f\"\u2713 plans.json moved to: {plans_dest}\")\n",
    "        else:\n",
    "            print(f\"\u26a0 plans.json not found in {trained_model_folder}\")\n",
    "\n",
    "        # Copy dataset.json\n",
    "        dataset_src = trained_model_folder / \"dataset.json\"\n",
    "        dataset_dest = expected_folder.parent / \"dataset.json\"\n",
    "        if dataset_src.exists():\n",
    "            shutil.copy2(dataset_src, dataset_dest)\n",
    "            print(f\"\u2713 dataset.json moved to: {dataset_dest}\")\n",
    "        else:\n",
    "            print(f\"\u26a0 dataset.json not found in {trained_model_folder}\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"\u26a0 Trained model folder not found at {trained_model_folder}\")\n",
    "        # Fallback check for dataset.json in preprocessed\n",
    "        dataset_json_pre = Path(PREPROCESSED_BASE) / dataset_name / \"dataset.json\"\n",
    "        if dataset_json_pre.exists():\n",
    "            dataset_dest = expected_folder.parent / \"dataset.json\"\n",
    "            shutil.copy2(dataset_json_pre, dataset_dest)\n",
    "            print(f\"\u2713 dataset.json (fallback: from preprocessed) moved to: {dataset_dest}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\u274c ERROR: Checkpoint not found at {CHECKPOINT_SOURCE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f2aaac",
   "metadata": {},
   "source": [
    "#### **Cell 5: Run Inference**\n",
    "\n",
    "This executes the prediction.\n",
    "\n",
    "* `-i`: Input folder (imagesTs)\n",
    "* `-o`: Output folder\n",
    "* `-d`: Dataset ID (002)\n",
    "* `-c`: Configuration (3d_fullres)\n",
    "* `-tr`: Your Custom Trainer name\n",
    "* `-f`: Fold (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db39212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Inference... (This may take a while depending on GPU)\")\n",
    "print(\"-------------------------------------------------------------\")\n",
    "\n",
    "!nnUNetv2_predict \\\n",
    "    -i {INPUT_IMAGES_FOLDER} \\\n",
    "    -o {OUTPUT_FOLDER} \\\n",
    "    -d 002 \\\n",
    "    -c 3d_fullres \\\n",
    "    -f {FOLD} \\\n",
    "    -tr nnUNetTrainerOversampling \\\n",
    "    -p nnUNetPlans \\\n",
    "    --save_probabilities # Optional: Remove if you only want the segmentation map\n",
    "\n",
    "print(\"\\n\u2713 Inference Complete!\")\n",
    "print(f\"Results saved to: {OUTPUT_FOLDER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad50fee9",
   "metadata": {},
   "source": [
    "#### **Cell 6: Visualize Results (Sanity Check)**\n",
    "\n",
    "Display a random slice of the input and the predicted output to verify the model is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b21f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_prediction(case_id):\n",
    "    # Paths\n",
    "    img_path = Path(INPUT_IMAGES_FOLDER) / f\"{case_id}_0000.nii.gz\"\n",
    "    # If .nii.gz doesn't exist, try .nii\n",
    "    if not img_path.exists():\n",
    "        img_path = Path(INPUT_IMAGES_FOLDER) / f\"{case_id}_0000.nii\"\n",
    "        \n",
    "    pred_path = Path(OUTPUT_FOLDER) / f\"{case_id}.nii.gz\"\n",
    "    if not pred_path.exists():\n",
    "        pred_path = Path(OUTPUT_FOLDER) / f\"{case_id}.nii\"\n",
    "\n",
    "    if not img_path.exists() or not pred_path.exists():\n",
    "        print(f\"Could not find files for {case_id}\")\n",
    "        return\n",
    "\n",
    "    # Load\n",
    "    img = nib.load(img_path).get_fdata()\n",
    "    pred = nib.load(pred_path).get_fdata()\n",
    "\n",
    "    # Find a slice with segmentation\n",
    "    if np.sum(pred) > 0:\n",
    "        # Get center of mass of segmentation\n",
    "        coords = np.where(pred > 0)\n",
    "        slice_idx = int(np.mean(coords[2]))\n",
    "        print(f\"Visualizing slice {slice_idx} (contains prediction)\")\n",
    "    else:\n",
    "        slice_idx = img.shape[2] // 2\n",
    "        print(f\"Visualizing middle slice {slice_idx} (Empty prediction)\")\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(np.rot90(img[:, :, slice_idx]), cmap='gray')\n",
    "    plt.title(f\"Input: {case_id}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(np.rot90(img[:, :, slice_idx]), cmap='gray')\n",
    "    plt.imshow(np.rot90(pred[:, :, slice_idx]), cmap='jet', alpha=0.5)\n",
    "    plt.title(\"Prediction Overlay\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    save_path = Path(OUTPUT_FOLDER) / f\"{case_id}_prediction.png\"\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\u2713 Saved visualization to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# List processed files\n",
    "predicted_files = [f.name.split('.')[0] for f in Path(OUTPUT_FOLDER).glob(\"*.nii*\") if not f.name.endswith('.json')]\n",
    "\n",
    "if len(predicted_files) > 0:\n",
    "    # Show first 3 cases\n",
    "    for case in predicted_files[:3]:\n",
    "        show_prediction(case)\n",
    "else:\n",
    "    print(\"No predictions found to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3753caab",
   "metadata": {},
   "source": [
    "#### **Cell 7: Quantitative Evaluation (Dice, HD95, etc.)**\n",
    "\n",
    "This section compares the model's predictions against the ground truth labels to calculate rigorous performance metrics.\n",
    "\n",
    "**Metrics Calculated:**\n",
    "- **Dice Score:** Overlap measure (higher is better, max 1.0).\n",
    "- **IoU (Jaccard):** Intersection over Union (higher is better, max 1.0).\n",
    "- **Precision & Recall:** correctness vs completeness.\n",
    "- **HD95 (Hausdorff Distance 95%):** Distance error in mm (lower is better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb7ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 1. Define Ground Truth Path\n",
    "# ---------------------------------------------------------\n",
    "# Assuming standard nnU-Net structure where labelsTs is parallel to imagesTs\n",
    "# If your path is different, please update LABELS_FOLDER manually.\n",
    "LABELS_FOLDER = INPUT_IMAGES_FOLDER.replace(\"imagesTs\", \"labelsTs\")\n",
    "\n",
    "print(f\"\u2713 Looking for Ground Truth in: {LABELS_FOLDER}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Define Metric Functions\n",
    "# ---------------------------------------------------------\n",
    "def calculate_metrics(pred_path, gt_path):\n",
    "    try:\n",
    "        # Load NIfTI files\n",
    "        pred_nii = nib.load(pred_path)\n",
    "        gt_nii = nib.load(gt_path)\n",
    "        \n",
    "        pred_data = pred_nii.get_fdata().astype(bool)\n",
    "        gt_data = gt_nii.get_fdata().astype(bool)\n",
    "        \n",
    "        # Voxel spacing for HD95 (read from header)\n",
    "        voxel_spacing = pred_nii.header.get_zooms()\n",
    "        \n",
    "        # Handle empty masks\n",
    "        if np.sum(pred_data) == 0 and np.sum(gt_data) == 0:\n",
    "            return {\n",
    "                \"Dice\": 1.0,\n",
    "                \"IoU\": 1.0,\n",
    "                \"Precision\": 1.0,\n",
    "                \"Recall\": 1.0,\n",
    "                \"HD95\": 0.0 # Perfect match (both empty)\n",
    "            }\n",
    "        elif np.sum(pred_data) == 0 or np.sum(gt_data) == 0:\n",
    "             return {\n",
    "                \"Dice\": 0.0,\n",
    "                \"IoU\": 0.0,\n",
    "                \"Precision\": 0.0,\n",
    "                \"Recall\": 0.0,\n",
    "                \"HD95\": np.nan # Undefined distance if one is empty\n",
    "            }\n",
    "            \n",
    "        # Calculate Metrics using MedPy\n",
    "        dice_score = dc(pred_data, gt_data)\n",
    "        iou_score = dice_score / (2 - dice_score) # Mathematical relation\n",
    "        precision_score = prec(pred_data, gt_data)\n",
    "        recall_score = rec(pred_data, gt_data)\n",
    "        hd95_score = hd95(pred_data, gt_data, voxelspacing=voxel_spacing)\n",
    "        \n",
    "        return {\n",
    "            \"Dice\": dice_score,\n",
    "            \"IoU\": iou_score,\n",
    "            \"Precision\": precision_score,\n",
    "            \"Recall\": recall_score,\n",
    "            \"HD95\": hd95_score\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating metrics for {pred_path.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Run Evaluation Loop\n",
    "# ---------------------------------------------------------\n",
    "results = []\n",
    "\n",
    "# Search for both .nii.gz and .nii\n",
    "pred_files = list(Path(OUTPUT_FOLDER).glob(\"*.nii.gz\")) + list(Path(OUTPUT_FOLDER).glob(\"*.nii\"))\n",
    "\n",
    "if not Path(LABELS_FOLDER).exists():\n",
    "    print(f\"\u274c Error: Labels folder not found at {LABELS_FOLDER}. Cannot run evaluation.\")\n",
    "elif len(pred_files) == 0:\n",
    "    print(f\"\u274c No predictions found in {OUTPUT_FOLDER}\")\n",
    "    print(\"Debug: Listing ALL files in output folder to help diagnosis:\")\n",
    "    try:\n",
    "        all_files = os.listdir(OUTPUT_FOLDER)\n",
    "        print(all_files if all_files else \"  (Folder is empty)\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error reading folder: {e}\")\n",
    "        \n",
    "    print(\"\\nPossible reasons:\")\n",
    "    print(\"1. Inference (Cell 5) failed or didn't run.\")\n",
    "    print(\"2. Output path mismatch.\")\n",
    "else:\n",
    "    print(f\"Starting evaluation on {len(pred_files)} cases...\")\n",
    "    \n",
    "    for pred_file in pred_files:\n",
    "        # Match Prediction filename to Ground Truth filename\n",
    "        case_id = pred_file.name\n",
    "        gt_file = Path(LABELS_FOLDER) / case_id\n",
    "        \n",
    "        # Fallback: sometimes GT is .nii while pred is .nii.gz or vice versa\n",
    "        if not gt_file.exists():\n",
    "             if case_id.endswith('.nii.gz'):\n",
    "                 gt_file = Path(LABELS_FOLDER) / case_id.replace('.nii.gz', '.nii')\n",
    "             elif case_id.endswith('.nii'):\n",
    "                 gt_file = Path(LABELS_FOLDER) / case_id.replace('.nii', '.nii.gz')\n",
    "        \n",
    "        if gt_file.exists():\n",
    "            metrics = calculate_metrics(pred_file, gt_file)\n",
    "            if metrics:\n",
    "                metrics['Case_ID'] = case_id.replace('.nii.gz', '').replace('.nii', '')\n",
    "                results.append(metrics)\n",
    "        else:\n",
    "            print(f\"\u26a0 Missing ground truth for {case_id} (Looked for: {gt_file.name})\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 4. Display & Save Results\n",
    "    # ---------------------------------------------------------\n",
    "    if len(results) > 0:\n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        # Reorder columns\n",
    "        cols = ['Case_ID', 'Dice', 'IoU', 'Precision', 'Recall', 'HD95']\n",
    "        df = df[cols]\n",
    "        \n",
    "        print(\"\\n\ud83c\udfc6 Evaluation Results:\")\n",
    "        print(df.to_string(index=False))\n",
    "        \n",
    "        # Summary Statistics\n",
    "        print(\"\\n\ud83d\udcca Summary Statistics:\")\n",
    "        summary = df.describe().loc[['mean', 'std', 'min', 'max']]\n",
    "        print(summary)\n",
    "        \n",
    "        # Save Summary\n",
    "        save_summary_path = Path(OUTPUT_FOLDER) / \"evaluation_summary.csv\"\n",
    "        summary.to_csv(save_summary_path)\n",
    "        print(f\"\\n\u2713 Summary Statistics saved to: {save_summary_path}\")\n",
    "        \n",
    "        # Save Metrics\n",
    "        save_csv_path = Path(OUTPUT_FOLDER) / \"evaluation_metrics.csv\"\n",
    "        df.to_csv(save_csv_path, index=False)\n",
    "        print(f\"\u2713 Metrics saved to: {save_csv_path}\")\n",
    "    else:\n",
    "        print(\"No matched results to evaluate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Instructions for Use:**\n",
    "\n",
    "1. **Copy Path:** Before running, copy the file path of your trained checkpoint (e.g., from your previous notebook output or uploaded dataset) and paste it into **Cell 2** (`CHECKPOINT_SOURCE`).\n",
    "2. **Dataset Location:** Ensure the `INPUT_IMAGES_FOLDER` in Cell 2 matches exactly where your test images (`imagesTs`) are located. I used the path derived from your file tree, but if you re-upload the dataset, the path might change slightly.\n",
    "3. **Run All:** Execute the cells in order. The logic in Cell 4 (\"Reconstruct Results Folder\") is the key to making this work without re-training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}